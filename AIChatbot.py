import streamlit as st
import os
from typing import List, Tuple, Dict, Any
from pinecone import Pinecone
from langchain_anthropic import ChatAnthropic
from langchain_openai import OpenAIEmbeddings
from langchain_core.documents import Document
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableLambda, RunnableParallel, RunnablePassthrough
from langchain_pinecone import PineconeVectorStore
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import time

# API í‚¤ ì„¤ì •
os.environ["ANTHROPIC_API_KEY"] = st.secrets["anthropic_api_key"]
os.environ["OPENAI_API_KEY"] = st.secrets["openai_api_key"]
os.environ["PINECONE_API_KEY"] = st.secrets["pinecone_api_key"]

class ModifiedPineconeVectorStore(PineconeVectorStore):
    def __init__(self, index, embedding, text_key: str = "text", namespace: str = ""):
        super().__init__(index, embedding, text_key, namespace)
        self.index = index
        self._embedding = embedding
        self._text_key = text_key
        self._namespace = namespace

    def similarity_search_with_score_by_vector(
        self, embedding: List[float], k: int = 8, filter: Dict[str, Any] = None, namespace: str = None
    ) -> List[Tuple[Document, float]]:
        namespace = namespace or self._namespace
        results = self.index.query(
            vector=embedding,
            top_k=k,
            include_metadata=True,
            include_values=True,
            filter=filter,
            namespace=namespace,
        )
        return [
            (
                Document(
                    page_content=result["metadata"].get(self._text_key, ""),
                    metadata={k: v for k, v in result["metadata"].items() if k != self._text_key}
                ),
                result["score"],
            )
            for result in results["matches"]
        ]

    def max_marginal_relevance_search_by_vector(
        self, embedding: List[float], k: int = 8, fetch_k: int = 30,
        lambda_mult: float = 0.7, filter: Dict[str, Any] = None, namespace: str = None
    ) -> List[Document]:
        namespace = namespace or self._namespace
        results = self.index.query(
            vector=embedding,
            top_k=fetch_k,
            include_metadata=True,
            include_values=True,
            filter=filter,
            namespace=namespace,
        )
        if not results['matches']:
            return []

        embeddings = [match['values'] for match in results['matches']]
        mmr_selected = maximal_marginal_relevance(
            np.array(embedding, dtype=np.float32),
            embeddings,
            k=min(k, len(results['matches'])),
            lambda_mult=lambda_mult
        )

        return [
            Document(
                page_content=results['matches'][i]['metadata'].get(self._text_key, ""),
                metadata={
                    'source': results['matches'][i]['metadata'].get('source', '').replace('C:\\Users\\minje\\data\\', '') if 'source' in results['matches'][i]['metadata'] else 'Unknown'
                }
            )
            for i in mmr_selected
        ]

def maximal_marginal_relevance(
    query_embedding: np.ndarray,
    embedding_list: List[np.ndarray],
    k: int = 4,
    lambda_mult: float = 0.5
) -> List[int]:
    similarity_scores = cosine_similarity([query_embedding], embedding_list)[0]
    selected_indices = []
    candidate_indices = list(range(len(embedding_list)))
    for _ in range(k):
        if not candidate_indices:
            break

        mmr_scores = [
            lambda_mult * similarity_scores[i] - (1 - lambda_mult) * max(
                [cosine_similarity([embedding_list[i]], [embedding_list[s]])[0][0] for s in selected_indices] or [0]
            )
            for i in candidate_indices
        ]
        max_index = candidate_indices[np.argmax(mmr_scores)]
        selected_indices.append(max_index)
        candidate_indices.remove(max_index)
    return selected_indices

def main():
    st.title("ğŸ¤Conference Q&A System")

    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    if "messages" not in st.session_state:
        st.session_state.messages = []
    if "mode" not in st.session_state:
        st.session_state.mode = "Report Mode"

    # Pinecone ì´ˆê¸°í™”
    pc = Pinecone(api_key=os.getenv("PINECONE_API_KEY"))
    index_name = "conference"
    index = pc.Index(index_name)

    # Claude ëª¨ë¸ ì„ íƒ
    if "claude_model" not in st.session_state:
        st.session_state.claude_model = "claude-3-opus-20240229"

    st.session_state.claude_model = st.selectbox(
        "Select Claude model:",
        ("claude-3-opus-20240229", "claude-3-sonnet-20240229"),
        index=("claude-3-opus-20240229", "claude-3-sonnet-20240229").index(st.session_state.claude_model)
    )
    llm = ChatAnthropic(model=st.session_state.claude_model)

    # Pinecone ë²¡í„° ìŠ¤í† ì–´ ì„¤ì •
    vectorstore = ModifiedPineconeVectorStore(
        index=index,
        embedding=OpenAIEmbeddings(model="text-embedding-ada-002"),
        text_key="source"
    )

    # ê²€ìƒ‰ê¸° ì„¤ì •
    retriever = vectorstore.as_retriever(
        search_type='mmr',
        search_kwargs={"k": 10, "fetch_k": 20, "lambda_mult": 0.7}
    )

    # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì • (ë¦¬í¬íŠ¸ ëª¨ë“œì™€ ì±—ë´‡ ëª¨ë“œ)
    report_template = """
    Human: Please write a comprehensive and informative report based on the following question and context. 
    The report should be approximately 4000 words in total, following this structure:

    1. Conference Overview (about 1000 words)
       - Explain the overall context of the conference
       - List the main topics, providing a brief 1-2 sentence explanation for each
       - Mention speakers or company names where possible
       - Mention the scale of the conference (number of attendees, number of presentation sessions, etc.) and its importance
       - Summarize in 1-2 sentences the impact or significance of this conference on the industry

    2. Analysis of Key Content (about 2000 words)
       - Analyze the key content discussed at the conference and reference sources
       - For each key session or topic:
         Topic:
         Fact: {{1. Provide a detailed description of approximately 5 sentences. 2. Include specific examples, numerical data, or case studies mentioned in the session}}
         Your opinion: {{Provide a detailed description of approximately 3 sentences}}
         Source: {{Show 2~3 data sources for each key topic}}

    3. Conclusion and Insights (about 1000 words)
       - Summarize new trends based on the conference content
       - Present derived insights from the conference that relate to the user's question. Focus on forward-looking perspectives and industry developments
       - Suggest 3 follow-up questions that the LG Group representative might ask, and provide brief answers to each (3~4 sentences)

    WRITING GUIDELINES:
    - USE THE PROVIDED CONTEXT TO ANSWER THE QUESTION
    - IF YOU DON'T KNOW THE ANSWER, ADMIT IT HONESTLY
    - WRITE IN KOREAN
    - ADHERE TO THE LENGTH CONSTRAINTS FOR EACH SECTION
    - USE THE STYLE OF HARVARD BUSINESS REVIEW (HBR): CLEAR, CONCISE, AND ANALYTICAL WRITING TARGETED AT BUSINESS EXECUTIVES
    - EMPLOY A PROFESSIONAL TONE WHILE MAINTAINING READABILITY
    - USE RELEVANT BUSINESS TERMINOLOGY AND CONCEPTS WHERE APPROPRIATE
    - INCLUDE DATA-DRIVEN INSIGHTS AND ACTIONABLE RECOMMENDATIONS

    Question: {{question}}
    Context: {{context}}

    Assistant: ë„¤, ì£¼ì–´ì§„ ì§€ì¹¨ì— ë”°ë¼ Harvard Business Review ìŠ¤íƒ€ì¼ë¡œ ì¢…í•©ì ì´ê³  ì •ë³´ê°€ í’ë¶€í•œ ë³´ê³ ì„œë¥¼ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ê² ìŠµë‹ˆë‹¤.

    Human: ì´ì œ ìœ„ì˜ ì§€ì¹¨ì— ë”°ë¼ ì•½ 4000ë‹¨ì–´ ë¶„ëŸ‰ì˜ ë³´ê³ ì„œë¥¼ ì‘ì„±í•´ ì£¼ì„¸ìš”.

    Assistant: [ë³´ê³ ì„œ ë‚´ìš©]
    """
    report_prompt = ChatPromptTemplate.from_template(report_template)

    chatbot_template = """
    Human: ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•´ ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì•½ 1,000ìë¡œ ëŒ€í™”ì²´ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”. í•œêµ­ì–´ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”.

    Question: {{question}}
    Context: {{context}}

    Assistant: ë„¤, ì£¼ì–´ì§„ ì§ˆë¬¸ì— ëŒ€í•´ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì•½ 1,000ì ë¶„ëŸ‰ì˜ ëŒ€í™”ì²´ ë‹µë³€ì„ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ê² ìŠµë‹ˆë‹¤.

    [ì±—ë´‡ ì‘ë‹µ ë‚´ìš©]
    """
    chatbot_prompt = ChatPromptTemplate.from_template(chatbot_template)

    def format_docs(docs: List[Document]) -> str:
        formatted = []
        for doc in docs:
            source = doc.metadata.get('source', 'Unknown source')
            formatted.append(f"Source: {source}")
        return "\n\n" + "\n\n".join(formatted)

    def get_report_chain(prompt):
        return (
            RunnableParallel(
                {"question": RunnablePassthrough(), "docs": retriever}
            )
            .assign(context=lambda x: format_docs(x["docs"]))
            .assign(answer=prompt | llm | StrOutputParser())
            .pick(["answer", "docs"])
        )

    def get_chatbot_chain(prompt):
        return (
            RunnableParallel(
                {"question": RunnablePassthrough(), "docs": retriever}
            )
            .assign(context=lambda x: format_docs(x["docs"]))
            .assign(answer=prompt | llm | StrOutputParser())
            .pick("answer")
        )

    report_chain = get_report_chain(report_prompt)
    chatbot_chain = get_chatbot_chain(chatbot_prompt)

    # ëª¨ë“œ ì„ íƒ
    new_mode = st.radio("ëª¨ë“œ ì„ íƒ:", ("Report Mode", "Chatbot Mode"), key="mode_selection")
    if new_mode != st.session_state.mode:
        st.session_state.mode = new_mode
        st.session_state.messages = []  # ëª¨ë“œ ë³€ê²½ ì‹œ ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”
        st.rerun()

    # í˜„ì¬ ëª¨ë“œ í‘œì‹œ
    st.write(f"í˜„ì¬ ëª¨ë“œ: {st.session_state.mode}")

    # ëŒ€í™” ì´ˆê¸°í™” í•¨ìˆ˜
    def reset_conversation():
        st.session_state.messages = []
        st.rerun()

    # ë¦¬ì…‹ í‚¤ì›Œë“œ í™•ì¸
    reset_keywords = ["ì²˜ìŒìœ¼ë¡œ", "ì´ˆê¸°í™”", "ë‹¤ì‹œ", "ì•ˆë…•"]

    # ëŒ€í™” ê¸°ë¡ í‘œì‹œ
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # ì‚¬ìš©ì ì…ë ¥
    if question := st.chat_input("ì»¨í¼ëŸ°ìŠ¤ì— ëŒ€í•´ ì§ˆë¬¸í•´ ì£¼ì„¸ìš”:"):
        # ë¦¬ì…‹ í‚¤ì›Œë“œ í™•ì¸
        if any(keyword in question for keyword in reset_keywords):
            reset_conversation()
        else:
            st.session_state.messages.append({"role": "user", "content": question})
            with st.chat_message("user"):
                st.markdown(question)

            with st.chat_message("assistant"):
                status_placeholder = st.empty()
                progress_bar = st.progress(0)

                try:
                    status_placeholder.text("ì¿¼ë¦¬ ì²˜ë¦¬ ì¤‘...")
                    progress_bar.progress(25)
                    time.sleep(1)

                    status_placeholder.text("ë°ì´í„°ë² ì´ìŠ¤ ê²€ìƒ‰ ì¤‘...")
                    progress_bar.progress(50)
                    chain = report_chain if st.session_state.mode == "Report Mode" else chatbot_chain
                    response = chain.invoke({"question": question})
                    time.sleep(1)

                    status_placeholder.text("ë‹µë³€ ìƒì„± ì¤‘...")
                    progress_bar.progress(75)
                    if st.session_state.mode == "Report Mode":
                        answer = response['answer']
                        sources = response['docs']
                    else:
                        answer = response
                        sources = []
                    time.sleep(1)

                    status_placeholder.text("ì‘ë‹µ ë§ˆë¬´ë¦¬ ì¤‘...")
                    progress_bar.progress(100)
                    time.sleep(0.5)

                except Exception as e:
                    st.error(f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
                    answer = "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”."
                    sources = []

                finally:
                    status_placeholder.empty()
                    progress_bar.empty()

                st.markdown(answer)

                if st.session_state.mode == "Report Mode" and sources:
                    with st.expander("Sources"):
                        for doc in sources:
                            st.write(f"- {doc.metadata['source']}")

                st.session_state.messages.append({"role": "assistant", "content": answer})

    # ëŒ€í™” ì´ˆê¸°í™” ë²„íŠ¼ ì¶”ê°€
    if st.button("ëŒ€í™” ì´ˆê¸°í™”"):
        reset_conversation()

if __name__ == "__main__":
    main()
