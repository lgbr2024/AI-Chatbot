import streamlit as st
import os
from typing import List, Tuple, Dict, Any
from pinecone import Pinecone
from langchain_anthropic import ChatAnthropic
from langchain_openai import OpenAIEmbeddings
from langchain_core.documents import Document
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableLambda, RunnableParallel, RunnablePassthrough
from langchain_pinecone import PineconeVectorStore
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import time
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.DEBUG)

# API í‚¤ ì„¤ì •
os.environ["ANTHROPIC_API_KEY"] = st.secrets["anthropic_api_key"]
os.environ["OPENAI_API_KEY"] = st.secrets["openai_api_key"]
os.environ["PINECONE_API_KEY"] = st.secrets["pinecone_api_key"]

# ModifiedPineconeVectorStore í´ë˜ìŠ¤ ì •ì˜
class ModifiedPineconeVectorStore(PineconeVectorStore):
    def __init__(self, index, embedding, text_key: str = "text", namespace: str = ""):
        super().__init__(index, embedding, text_key, namespace)
        self.index = index
        self._embedding = embedding
        self._text_key = text_key
        self._namespace = namespace

    def similarity_search_with_score_by_vector(
        self, embedding: List[float], k: int = 8, filter: Dict[str, Any] = None, namespace: str = None
    ) -> List[Tuple[Document, float]]:
        namespace = namespace or self._namespace
        results = self.index.query(
            vector=embedding,
            top_k=k,
            include_metadata=True,
            include_values=True,
            filter=filter,
            namespace=namespace,
        )
        return [
            (
                Document(
                    page_content=result["metadata"].get(self._text_key, ""),
                    metadata={k: v for k, v in result["metadata"].items() if k != self._text_key}
                ),
                result["score"],
            )
            for result in results["matches"]
        ]

    def max_marginal_relevance_search_by_vector(
        self, embedding: List[float], k: int = 8, fetch_k: int = 30,
        lambda_mult: float = 0.7, filter: Dict[str, Any] = None, namespace: str = None
    ) -> List[Document]:
        namespace = namespace or self._namespace
        results = self.index.query(
            vector=embedding,
            top_k=fetch_k,
            include_metadata=True,
            include_values=True,
            filter=filter,
            namespace=namespace,
        )
        if not results['matches']:
            return []

        embeddings = [match['values'] for match in results['matches']]
        mmr_selected = maximal_marginal_relevance(
            np.array(embedding, dtype=np.float32),
            embeddings,
            k=min(k, len(results['matches'])),
            lambda_mult=lambda_mult
        )

        return [
            Document(
                page_content=results['matches'][i]['metadata'].get(self._text_key, ""),
                metadata={
                    'source': results['matches'][i]['metadata'].get('source', '').replace('C:\\Users\\minje\\data\\', '') if 'source' in results['matches'][i]['metadata'] else 'Unknown'
                }
            )
            for i in mmr_selected
        ]

def maximal_marginal_relevance(
    query_embedding: np.ndarray,
    embedding_list: List[np.ndarray],
    k: int = 4,
    lambda_mult: float = 0.5
) -> List[int]:
    similarity_scores = cosine_similarity([query_embedding], embedding_list)[0]
    selected_indices = []
    candidate_indices = list(range(len(embedding_list)))
    for _ in range(k):
        if not candidate_indices:
            break

        mmr_scores = [
            lambda_mult * similarity_scores[i] - (1 - lambda_mult) * max(
                [cosine_similarity([embedding_list[i]], [embedding_list[s]])[0][0] for s in selected_indices] or [0]
            )
            for i in candidate_indices
        ]
        max_index = candidate_indices[np.argmax(mmr_scores)]
        selected_indices.append(max_index)
        candidate_indices.remove(max_index)
    return selected_indices

def format_docs(docs: Any) -> str:
    logging.debug(f"format_docs í•¨ìˆ˜ê°€ ë°›ì€ ë°ì´í„° ìœ í˜•: {type(docs)}")
    logging.debug(f"docsì˜ ë‚´ìš©: {docs}")
    
    formatted = []
    
    if isinstance(docs, list):
        for doc in docs:
            logging.debug(f"ì²˜ë¦¬ ì¤‘ì¸ docì˜ ìœ í˜•: {type(doc)}")
            if isinstance(doc, Document):
                source = doc.metadata.get('source', 'ì•Œ ìˆ˜ ì—†ëŠ” ì¶œì²˜')
            elif isinstance(doc, dict):
                source = doc.get('metadata', {}).get('source', 'ì•Œ ìˆ˜ ì—†ëŠ” ì¶œì²˜')
            elif isinstance(doc, str):
                source = doc  # ë¬¸ìì—´ì¸ ê²½ìš°, ê·¸ëŒ€ë¡œ ì¶œë ¥
            else:
                source = f"ì•Œ ìˆ˜ ì—†ëŠ” ì¶œì²˜ (ìœ í˜•: {type(doc)})"
            logging.debug(f"ì¶”ì¶œëœ ì¶œì²˜: {source}")
            formatted.append(f"ì¶œì²˜: {source}")
    elif isinstance(docs, dict):
        source = docs.get('metadata', {}).get('source', 'ì•Œ ìˆ˜ ì—†ëŠ” ì¶œì²˜')
        formatted.append(f"ì¶œì²˜: {source}")
    elif isinstance(docs, str):
        formatted.append(f"ì¶œì²˜: {docs}")  # ë¬¸ìì—´ì¸ ê²½ìš°, ê·¸ëŒ€ë¡œ ì¶œë ¥
    else:
        formatted.append(f"ì•Œ ìˆ˜ ì—†ëŠ” í˜•ì‹ì˜ ë¬¸ì„œ (ìœ í˜•: {type(docs)})")
    
    return "\n\n" + "\n\n".join(formatted)


def main():
    st.title("ğŸ¤Conference Q&A System")

    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    if "messages" not in st.session_state:
        st.session_state.messages = []
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []
    if "mode" not in st.session_state:
        st.session_state.mode = "Report Mode"

    # Pinecone ì´ˆê¸°í™”
    try:
        pc = Pinecone(api_key=os.getenv("PINECONE_API_KEY"))
        index_name = "itconference"
        index = pc.Index(index_name)
    except Exception as e:
        logging.error(f"Pinecone ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
        st.error("Pinecone ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”.")
        return

    # Claude ëª¨ë¸ ì„ íƒ
    if "claude_model" not in st.session_state:
        st.session_state.claude_model = "claude-3-opus-20240229"

    st.session_state.claude_model = st.selectbox(
        "Select Claude model:",
        ("claude-3-opus-20240229", "claude-3-sonnet-20240229"),
        index=("claude-3-opus-20240229", "claude-3-sonnet-20240229").index(st.session_state.claude_model)
    )
    llm = ChatAnthropic(model=st.session_state.claude_model)

    # Pinecone ë²¡í„° ìŠ¤í† ì–´ ì„¤ì •
    try:
        vectorstore = ModifiedPineconeVectorStore(
            index=index,
            embedding=OpenAIEmbeddings(model="text-embedding-ada-002"),
            text_key="source"
        )
    except Exception as e:
        logging.error(f"ë²¡í„° ìŠ¤í† ì–´ ì„¤ì • ì˜¤ë¥˜: {e}")
        st.error("ë²¡í„° ìŠ¤í† ì–´ ì„¤ì • ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”.")
        return

    # ê²€ìƒ‰ê¸° ì„¤ì •
    retriever = vectorstore.as_retriever(
        search_type='mmr',
        search_kwargs={"k": 10, "fetch_k": 20, "lambda_mult": 0.7}
    )

    # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì • (ë¦¬í¬íŠ¸ ëª¨ë“œì™€ ì±—ë´‡ ëª¨ë“œ)
    report_template = """
    Human: ë‹¤ìŒ ì§ˆë¬¸ê³¼ ì»¨í…ìŠ¤íŠ¸, ê·¸ë¦¬ê³  ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì•½ 20,000ì ë¶„ëŸ‰ì˜ ì¢…í•©ì ì¸ ë³´ê³ ì„œë¥¼ ì‘ì„±í•´ ì£¼ì„¸ìš”. Harvard Business Review (HBR) ìŠ¤íƒ€ì¼ì„ ì‚¬ìš©í•˜ê³  ë‹¤ìŒ ì§€ì¹¨ì„ ë”°ë¼ì£¼ì„¸ìš”:

    ì§ˆë¬¸: {question}
    ì»¨í…ìŠ¤íŠ¸: {context}
    ì´ì „ ëŒ€í™” ë‚´ìš©: {chat_history}

    ì§€ì¹¨:
    1. ì»¨í¼ëŸ°ìŠ¤ ê°œìš”ë¡œ ì‹œì‘í•˜ì—¬ ë§¥ë½ê³¼ ì£¼ìš” ì£¼ì œë¥¼ ì„¤ëª…í•˜ì„¸ìš”.
    2. ì»¨í¼ëŸ°ìŠ¤ì˜ ì£¼ìš” ë‚´ìš©ì„ ë¶„ì„í•˜ê³ , ì£¼ì œ, ì‚¬ì‹¤, ê·€í•˜ì˜ ì˜ê²¬ìœ¼ë¡œ ë¶„ë¥˜í•˜ì„¸ìš”.
    3. ìƒˆë¡œìš´ íŠ¸ë Œë“œ, ì¸ì‚¬ì´íŠ¸, ê·¸ë¦¬ê³  ê°„ë‹¨í•œ ë‹µë³€ì´ í¬í•¨ëœ 3ê°€ì§€ í›„ì† ì§ˆë¬¸ìœ¼ë¡œ ê²°ë¡ ì„ ë§ºìœ¼ì„¸ìš”.
    4. ê²½ì˜ì§„ì„ ëŒ€ìƒìœ¼ë¡œ í•œ ëª…í™•í•˜ê³  ê°„ê²°í•œ ë¹„ì¦ˆë‹ˆìŠ¤ ë¬¸ì²´ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
    5. í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ê³ , í’ë¶€í•œ ë¬¸ì¥ì„ ì‚¬ìš©í•˜ì—¬ ë‹µë³€ì˜ ì§ˆì„ ë†’ì´ì„¸ìš”.
    6. ì „ì²´ ë³´ê³ ì„œì˜ ê¸¸ì´ê°€ ì•½ 20,000ìê°€ ë˜ë„ë¡ ì‘ì„±í•´ ì£¼ì„¸ìš”.

    Assistant: ë„¤, ì´í•´í–ˆìŠµë‹ˆë‹¤. ì£¼ì–´ì§„ ì§ˆë¬¸, ì»¨í…ìŠ¤íŠ¸, ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì•½ 20,000ì ë¶„ëŸ‰ì˜ ì¢…í•©ì ì¸ ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ê² ìŠµë‹ˆë‹¤.

    [ë³´ê³ ì„œ ë‚´ìš©]

    Human: ê°ì‚¬í•©ë‹ˆë‹¤. ì´ì œ ì±—ë´‡ ëª¨ë“œë¥¼ ìœ„í•œ ê°„ë‹¨í•œ ì‘ë‹µì„ ìƒì„±í•´ ì£¼ì„¸ìš”. ì§ˆë¬¸ê³¼ ì»¨í…ìŠ¤íŠ¸ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

    Question: {question}
    Context: {context}

    Assistant: ë„¤, ì£¼ì–´ì§„ ì§ˆë¬¸ê³¼ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°„ë‹¨í•œ ì‘ë‹µì„ ìƒì„±í•˜ê² ìŠµë‹ˆë‹¤.

    [ì±—ë´‡ ì‘ë‹µ ë‚´ìš©]
    """
    report_prompt = ChatPromptTemplate.from_template(report_template)

    chatbot_template = """
    Human: ë‹¹ì‹ ì€ IT ì»¨í¼ëŸ°ìŠ¤ì— ëŒ€í•œ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ë‹¤ìŒì€ ì§€ê¸ˆê¹Œì§€ì˜ ëŒ€í™” ë‚´ìš©ì…ë‹ˆë‹¤:

    {chat_history}

    ì´ì œ ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•´ ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì•½ 10,000ìë¡œ ëŒ€í™”ì²´ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”. í•œêµ­ì–´ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”.

    Question: {question}
    Context: {context}

    Assistant: ë„¤, ì´í•´í–ˆìŠµë‹ˆë‹¤. ì§€ê¸ˆê¹Œì§€ì˜ ëŒ€í™” ë‚´ìš©ê³¼ ì£¼ì–´ì§„ ì§ˆë¬¸, ê·¸ë¦¬ê³  ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì•½ 10,000ì ë¶„ëŸ‰ì˜ ëŒ€í™”ì²´ ë‹µë³€ì„ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ê² ìŠµë‹ˆë‹¤.

    [ì±—ë´‡ ì‘ë‹µ ë‚´ìš©]
    """
    chatbot_prompt = ChatPromptTemplate.from_template(chatbot_template)

    format = RunnableLambda(format_docs)

    def get_report_chain(prompt):
        answer = prompt | llm | StrOutputParser()
        return (
            RunnableParallel(question=RunnablePassthrough(), docs=retriever)
            .assign(context=lambda x: format_docs(x['docs']))
            .assign(chat_history=lambda x: format_chat_history(st.session_state.chat_history))
            .assign(answer=answer)
            .pick(["answer", "docs"])
        )
    
    def get_chatbot_chain(prompt):
        answer = prompt | llm | StrOutputParser()
        return (
            RunnableParallel(question=RunnablePassthrough(), docs=retriever)
            .assign(context=lambda x: format_docs(x['docs']))
            .assign(chat_history=lambda x: format_chat_history(st.session_state.chat_history))
            .assign(answer=answer)
            .pick("answer")
        )

    def format_chat_history(chat_history):
        formatted = []
        for message in chat_history:
            role = "Human" if message["role"] == "user" else "Assistant"
            formatted.append(f"{role}: {message['content']}")
        return "\n".join(formatted)

    report_chain = get_report_chain(report_prompt)
    chatbot_chain = get_chatbot_chain(chatbot_prompt)

    # ëª¨ë“œ ì„ íƒ
    new_mode = st.radio("ëª¨ë“œ ì„ íƒ:", ("Report Mode", "Chatbot Mode"), key="mode_selection")
    if new_mode != st.session_state.mode:
        st.session_state.mode = new_mode
        st.session_state.messages = []  # ëª¨ë“œ ë³€ê²½ ì‹œ ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”
        st.session_state.chat_history = []  # ëª¨ë“œ ë³€ê²½ ì‹œ ì±„íŒ… íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”
        st.rerun()

    # í˜„ì¬ ëª¨ë“œ í‘œì‹œ
    st.write(f"í˜„ì¬ ëª¨ë“œ: {st.session_state.mode}")

    # ëŒ€í™” ì´ˆê¸°í™” í•¨ìˆ˜
    def reset_conversation():
        st.session_state.messages = []
        st.session_state.chat_history = []
        st.rerun()

    # ë¦¬ì…‹ í‚¤ì›Œë“œ í™•ì¸
    reset_keywords = ["ì²˜ìŒìœ¼ë¡œ", "ì´ˆê¸°í™”", "ë‹¤ì‹œ", "ì•ˆë…•"]

    # ëŒ€í™” ê¸°ë¡ í‘œì‹œ
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # ì‚¬ìš©ì ì…ë ¥
    if question := st.chat_input("ì»¨í¼ëŸ°ìŠ¤ì— ëŒ€í•´ ì§ˆë¬¸í•´ ì£¼ì„¸ìš”:"):
        # ë¦¬ì…‹ í‚¤ì›Œë“œ í™•ì¸
        if any(keyword in question for keyword in reset_keywords):
            reset_conversation()
        else:
            st.session_state.messages.append({"role": "user", "content": question})
            st.session_state.chat_history.append({"role": "user", "content": question})
            with st.chat_message("user"):
                st.markdown(question)

            with st.chat_message("assistant"):
                # ìƒíƒœ ì—…ë°ì´íŠ¸ë¥¼ ìœ„í•œ í”Œë ˆì´ìŠ¤í™€ë” ìƒì„±
                status_placeholder = st.empty()
                progress_bar = st.progress(0)

                try:
                    # ì¿¼ë¦¬ ì²˜ë¦¬
                    status_placeholder.text("ì¿¼ë¦¬ ì²˜ë¦¬ ì¤‘...")
                    progress_bar.progress(25)
                    time.sleep(1)  # ì²˜ë¦¬ ì‹œê°„ ì‹œë®¬ë ˆì´ì…˜

                    # ë°ì´í„°ë² ì´ìŠ¤ ê²€ìƒ‰
                    status_placeholder.text("ë°ì´í„°ë² ì´ìŠ¤ ê²€ìƒ‰ ì¤‘...")
                    progress_bar.progress(50)
                    chain = report_chain if st.session_state.mode == "Report Mode" else chatbot_chain
                    logging.debug(f"Using chain: {type(chain)}")
                    response = chain.invoke(question)
                    logging.debug(f"Chain response: {response}")
                    time.sleep(1)  # ê²€ìƒ‰ ì‹œê°„ ì‹œë®¬ë ˆì´ì…˜

                    # ë‹µë³€ ìƒì„±
                    status_placeholder.text("ë‹µë³€ ìƒì„± ì¤‘...")
                    progress_bar.progress(75)
                    answer = response['answer'] if st.session_state.mode == "Report Mode" else response
                    logging.debug(f"Generated answer: {answer[:100]}...")  # ë‹µë³€ì˜ ì²˜ìŒ 100ìë§Œ ë¡œê¹…
                    time.sleep(1)  # ìƒì„± ì‹œê°„ ì‹œë®¬ë ˆì´ì…˜

                    # ì‘ë‹µ ë§ˆë¬´ë¦¬
                    status_placeholder.text("ì‘ë‹µ ë§ˆë¬´ë¦¬ ì¤‘...")
                    progress_bar.progress(100)
                    time.sleep(0.5)  # ë§ˆë¬´ë¦¬ ì‹œê°„ ì‹œë®¬ë ˆì´ì…˜

                except Exception as e:
                    logging.exception(f"Error occurred during response generation: {e}")
                    st.error("ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ë™ì•ˆ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ ë¡œê·¸ë¥¼ í™•ì¸í•´ ì£¼ì„¸ìš”.")
                    return

                finally:
                    # ìƒíƒœ í‘œì‹œ ì œê±°
                    status_placeholder.empty()
                    progress_bar.empty()

                # ë‹µë³€ í‘œì‹œ
                st.markdown(answer)

                # ì†ŒìŠ¤ í‘œì‹œ (ë¦¬í¬íŠ¸ ëª¨ë“œë§Œ)
                if st.session_state.mode == "Report Mode":
                    with st.expander("Sources"):
                        if isinstance(response, dict) and 'docs' in response:
                            for doc in response['docs']:
                                if isinstance(doc, Document) and hasattr(doc, 'metadata'):
                                    st.write(f"- {doc.metadata.get('source', 'ì•Œ ìˆ˜ ì—†ëŠ” ì¶œì²˜')}")
                                else:
                                    st.write("- ì•Œ ìˆ˜ ì—†ëŠ” ì¶œì²˜")
                        else:
                            st.write("ì†ŒìŠ¤ ì •ë³´ë¥¼ í‘œì‹œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

                # ëŒ€í™” ê¸°ë¡ì— ë„ìš°ë¯¸ ì‘ë‹µ ì¶”ê°€
                st.session_state.messages.append({"role": "assistant", "content": answer})
                st.session_state.chat_history.append({"role": "assistant", "content": answer})

    # ëŒ€í™” ì´ˆê¸°í™” ë²„íŠ¼ ì¶”ê°€
    if st.button("ëŒ€í™” ì´ˆê¸°í™”"):
        reset_conversation()

if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        logging.exception(f"ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹¤í–‰ ì¤‘ ì˜ˆê¸°ì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
        st.error("ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”.")
