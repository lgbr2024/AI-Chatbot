import streamlit as st
import os
from typing import List, Tuple, Dict, Any
from pinecone import Pinecone
from langchain_anthropic import ChatAnthropic
from langchain_openai import OpenAIEmbeddings
from langchain_core.documents import Document
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableLambda, RunnableParallel, RunnablePassthrough
from langchain_pinecone import PineconeVectorStore
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import time
import logging
# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.DEBUG)
# API í‚¤ ì„¤ì •
os.environ["ANTHROPIC_API_KEY"] = st.secrets["anthropic_api_key"]
os.environ["OPENAI_API_KEY"] = st.secrets["openai_api_key"]
os.environ["PINECONE_API_KEY"] = st.secrets["pinecone_api_key"]
# ModifiedPineconeVectorStore í´ë˜ìŠ¤ ì •ì˜
class ModifiedPineconeVectorStore(PineconeVectorStore):
    def __init__(self, index, embedding, text_key: str = "text", namespace: str = ""):
        super().__init__(index, embedding, text_key, namespace)
        self.index = index
        self._embedding = embedding
        self._text_key = text_key
        self._namespace = namespace
    def similarity_search_with_score_by_vector(
        self, embedding: List[float], k: int = 8, filter: Dict[str, Any] = None, namespace: str = None
    ) -> List[Tuple[Document, float]]:
        namespace = namespace or self._namespace
        results = self.index.query(
            vector=embedding,
            top_k=k,
            include_metadata=True,
            include_values=True,
            filter=filter,
            namespace=namespace,
        )
        return [
            (
                Document(
                    page_content=result["metadata"].get(self._text_key, ""),
                    metadata={k: v for k, v in result["metadata"].items() if k != self._text_key}
                ),
                result["score"],
            )
            for result in results["matches"]
        ]
    def max_marginal_relevance_search_by_vector(
        self, embedding: List[float], k: int = 8, fetch_k: int = 30,
        lambda_mult: float = 0.7, filter: Dict[str, Any] = None, namespace: str = None
    ) -> List[Document]:
        namespace = namespace or self._namespace
        results = self.index.query(
            vector=embedding,
            top_k=fetch_k,
            include_metadata=True,
            include_values=True,
            filter=filter,
            namespace=namespace,
        )
        if not results['matches']:
            return []
        embeddings = [match['values'] for match in results['matches']]
        mmr_selected = maximal_marginal_relevance(
            np.array(embedding, dtype=np.float32),
            embeddings,
            k=min(k, len(results['matches'])),
            lambda_mult=lambda_mult
        )
        return [
            Document(
                page_content=results['matches'][i]['metadata'].get(self._text_key, ""),
                metadata={
                    'source': results['matches'][i]['metadata'].get('source', '').replace('C:\\Users\\minje\\data\\', '') if 'source' in results['matches'][i]['metadata'] else 'Unknown'
                }
            )
            for i in mmr_selected
        ]
def maximal_marginal_relevance(
    query_embedding: np.ndarray,
    embedding_list: List[np.ndarray],
    k: int = 4,
    lambda_mult: float = 0.5
) -> List[int]:
    similarity_scores = cosine_similarity([query_embedding], embedding_list)[0]
    selected_indices = []
    candidate_indices = list(range(len(embedding_list)))
    for _ in range(k):
        if not candidate_indices:
            break
        mmr_scores = [
            lambda_mult * similarity_scores[i] - (1 - lambda_mult) * max(
                [cosine_similarity([embedding_list[i]], [embedding_list[s]])[0][0] for s in selected_indices] or [0]
            )
            for i in candidate_indices
        ]
        max_index = candidate_indices[np.argmax(mmr_scores)]
        selected_indices.append(max_index)
        candidate_indices.remove(max_index)
    return selected_indices
def format_docs(docs: List[Document]) -> str:
    formatted = []
    for doc in docs:
        logging.debug(f"ë¬¸ì„œ ì²˜ë¦¬ ì¤‘: {type(doc)}")
        if isinstance(doc, Document):
            logging.debug(f"ë¬¸ì„œ ë©”íƒ€ë°ì´í„°: {doc.metadata if hasattr(doc, 'metadata') else 'ë©”íƒ€ë°ì´í„° ì—†ìŒ'}")
            source = doc.metadata.get('source', 'ì•Œ ìˆ˜ ì—†ëŠ” ì¶œì²˜') if hasattr(doc, 'metadata') else 'ì•Œ ìˆ˜ ì—†ëŠ” ì¶œì²˜'
        else:
            source = 'ì•Œ ìˆ˜ ì—†ëŠ” ì¶œì²˜'
        formatted.append(f"ì¶œì²˜: {source}")
    return "\n\n" + "\n\n".join(formatted)
def main():
    st.title("ğŸ¤Conference Q&A System")
    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    if "messages" not in st.session_state:
        st.session_state.messages = []
    if "mode" not in st.session_state:
        st.session_state.mode = "Report Mode"
    # Pinecone ì´ˆê¸°í™”
    try:
        pc = Pinecone(api_key=os.getenv("PINECONE_API_KEY"))
        index_name = "itconference"
        index = pc.Index(index_name)
    except Exception as e:
        logging.error(f"Pinecone ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
        st.error("Pinecone ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”.")
        return
    # Claude ëª¨ë¸ ì„ íƒ
    if "claude_model" not in st.session_state:
        st.session_state.claude_model = "claude-3-opus-20240229"
    st.session_state.claude_model = st.selectbox(
        "Select Claude model:",
        ("claude-3-opus-20240229", "claude-3-sonnet-20240229"),
        index=("claude-3-opus-20240229", "claude-3-sonnet-20240229").index(st.session_state.claude_model)
    )
    llm = ChatAnthropic(model=st.session_state.claude_model)
    # Pinecone ë²¡í„° ìŠ¤í† ì–´ ì„¤ì •
    try:
        vectorstore = ModifiedPineconeVectorStore(
            index=index,
            embedding=OpenAIEmbeddings(model="text-embedding-ada-002"),
            text_key="source"
        )
    except Exception as e:
        logging.error(f"ë²¡í„° ìŠ¤í† ì–´ ì„¤ì • ì˜¤ë¥˜: {e}")
        st.error("ë²¡í„° ìŠ¤í† ì–´ ì„¤ì • ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”.")
        return
    # ê²€ìƒ‰ê¸° ì„¤ì •
    retriever = vectorstore.as_retriever(
        search_type='mmr',
        search_kwargs={"k": 10, "fetch_k": 20, "lambda_mult": 0.7}
    )
    # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì • (ë¦¬í¬íŠ¸ ëª¨ë“œì™€ ì±—ë´‡ ëª¨ë“œ)
    report_template = """
    Human: Please provide a comprehensive report based on the following question and context. Use the style of Harvard Business Review (HBR) and follow these guidelines:
    Question: {question}
    Context: {context}
    Guidelines:
    1. Start with a conference overview, explaining the context and main themes.
    2. Analyze key content from the conference, categorizing it into topics, facts, and your opinions.
    3. Conclude with new trends, insights, and 3 follow-up questions with brief answers.
    4. Use clear and concise business writing targeted at executives.
    5. Answer in Korean and provide rich sentences to enhance the quality of the answer.
    6. Adhere to these length constraints: Conference Overview (ì•½ 2000 ë‹¨ì–´), Contents (ì•½ 18000 ë‹¨ì–´), Conclusion (ì•½ 2000 ë‹¨ì–´).
    Assistant: ë„¤, ì£¼ì–´ì§„ ì§€ì¹¨ì— ë”°ë¼ ì»¨í¼ëŸ°ìŠ¤ì— ëŒ€í•œ ì¢…í•©ì ì¸ ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ê² ìŠµë‹ˆë‹¤.
    Human: ì´ì œ ìœ„ì˜ ì§€ì¹¨ì— ë”°ë¼ ë³´ê³ ì„œë¥¼ ì‘ì„±í•´ ì£¼ì„¸ìš”.
    Assistant: [ë³´ê³ ì„œ ë‚´ìš©]
    Human: ê°ì‚¬í•©ë‹ˆë‹¤. ì´ì œ ì±—ë´‡ ëª¨ë“œë¥¼ ìœ„í•œ ê°„ë‹¨í•œ ì‘ë‹µì„ ìƒì„±í•´ ì£¼ì„¸ìš”. ì§ˆë¬¸ê³¼ ì»¨í…ìŠ¤íŠ¸ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:
    Question: {question}
    Context: {context}
    Assistant: ë„¤, ì£¼ì–´ì§„ ì§ˆë¬¸ê³¼ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°„ë‹¨í•œ ì‘ë‹µì„ ìƒì„±í•˜ê² ìŠµë‹ˆë‹¤.
    [ì±—ë´‡ ì‘ë‹µ ë‚´ìš©]
    """
    report_prompt = ChatPromptTemplate.from_template(report_template)

    chatbot_template = """
    Human: ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•´ ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì•½ 20,000ìë¡œ ëŒ€í™”ì²´ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”. í•œêµ­ì–´ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”.
    Question: {question}
    Context: {context}
    Assistant: ë„¤, ì£¼ì–´ì§„ ì§ˆë¬¸ì— ëŒ€í•´ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì•½ 20,000ì ë¶„ëŸ‰ì˜ ëŒ€í™”ì²´ ë‹µë³€ì„ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ê² ìŠµë‹ˆë‹¤.
    [ì±—ë´‡ ì‘ë‹µ ë‚´ìš©]
    """
    chatbot_prompt = ChatPromptTemplate.from_template(chatbot_template)
    format = RunnableLambda(format_docs)
    def get_report_chain(prompt):
        answer = prompt | llm | StrOutputParser()
        return (
            RunnableParallel(question=RunnablePassthrough(), docs=retriever)
            .assign(context=format)
            .assign(answer=answer)
            .pick(["answer", "docs"])
        )
    def get_chatbot_chain(prompt):
        answer = prompt | llm | StrOutputParser()
        return (
            RunnableParallel(question=RunnablePassthrough(), docs=retriever)
            .assign(context=format)
            .assign(answer=answer)
            .pick("answer")
        )
    report_chain = get_report_chain(report_prompt)
    chatbot_chain = get_chatbot_chain(chatbot_prompt)
    # ëª¨ë“œ ì„ íƒ
    new_mode = st.radio("ëª¨ë“œ ì„ íƒ:", ("Report Mode", "Chatbot Mode"), key="mode_selection")
    if new_mode != st.session_state.mode:
        st.session_state.mode = new_mode
        st.session_state.messages = []  # ëª¨ë“œ ë³€ê²½ ì‹œ ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”
        st.rerun()
    # í˜„ì¬ ëª¨ë“œ í‘œì‹œ
    st.write(f"í˜„ì¬ ëª¨ë“œ: {st.session_state.mode}")
    # ëŒ€í™” ì´ˆê¸°í™” í•¨ìˆ˜
    def reset_conversation():
        st.session_state.messages = []
        st.rerun()
    # ë¦¬ì…‹ í‚¤ì›Œë“œ í™•ì¸
    reset_keywords = ["ì²˜ìŒìœ¼ë¡œ", "ì´ˆê¸°í™”", "ë‹¤ì‹œ", "ì•ˆë…•"]
    # ëŒ€í™” ê¸°ë¡ í‘œì‹œ
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
    # ì‚¬ìš©ì ì…ë ¥
    if question := st.chat_input("ì»¨í¼ëŸ°ìŠ¤ì— ëŒ€í•´ ì§ˆë¬¸í•´ ì£¼ì„¸ìš”:"):
        # ë¦¬ì…‹ í‚¤ì›Œë“œ í™•ì¸
        if any(keyword in question for keyword in reset_keywords):
            reset_conversation()
        else:
            st.session_state.messages.append({"role": "user", "content": question})
            with st.chat_message("user"):
                st.markdown(question)
            with st.chat_message("assistant"):
                # ìƒíƒœ ì—…ë°ì´íŠ¸ë¥¼ ìœ„í•œ í”Œë ˆì´ìŠ¤í™€ë” ìƒì„±
                status_placeholder = st.empty()
                progress_bar = st.progress(0)
                try:
                    # ì¿¼ë¦¬ ì²˜ë¦¬
                    status_placeholder.text("ì¿¼ë¦¬ ì²˜ë¦¬ ì¤‘...")
                    progress_bar.progress(25)
                    time.sleep(1)  # ì²˜ë¦¬ ì‹œê°„ ì‹œë®¬ë ˆì´ì…˜
                    # ë°ì´í„°ë² ì´ìŠ¤ ê²€ìƒ‰
                    status_placeholder.text("ë°ì´í„°ë² ì´ìŠ¤ ê²€ìƒ‰ ì¤‘...")
                    progress_bar.progress(50)
                    chain = report_chain if st.session_state.mode == "Report Mode" else chatbot_chain
                    response = chain.invoke(question)
                    time.sleep(1)  # ê²€ìƒ‰ ì‹œê°„ ì‹œë®¬ë ˆì´ì…˜
                    # ë‹µë³€ ìƒì„±
                    status_placeholder.text("ë‹µë³€ ìƒì„± ì¤‘...")
                    progress_bar.progress(75)
                    answer = response['answer'] if st.session_state.mode == "Report Mode" else response
                    time.sleep(1)  # ìƒì„± ì‹œê°„ ì‹œë®¬ë ˆì´ì…˜
                    # ì‘ë‹µ ë§ˆë¬´ë¦¬
                    status_placeholder.text("ì‘ë‹µ ë§ˆë¬´ë¦¬ ì¤‘...")
                    progress_bar.progress(100)
                    time.sleep(0.5)  # ë§ˆë¬´ë¦¬ ì‹œê°„ ì‹œë®¬ë ˆì´ì…˜
                except Exception as e:
                    logging.error(f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                    st.error("ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ë™ì•ˆ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”.")
                    return
                finally:
                    # ìƒíƒœ í‘œì‹œ ì œê±°
                    status_placeholder.empty()
                    progress_bar.empty()
                # ë‹µë³€ í‘œì‹œ
                st.markdown(answer)
                # ì†ŒìŠ¤ í‘œì‹œ
                # ì†ŒìŠ¤ í‘œì‹œ (ë¦¬í¬íŠ¸ ëª¨ë“œë§Œ)
                if st.session_state.mode == "Report Mode":
                    with st.expander("Sources"):
                        if isinstance(response, dict) and 'docs' in response:
                            for doc in response['docs']:
                                if isinstance(doc, Document) and hasattr(doc, 'metadata'):
                                    st.write(f"- {doc.metadata.get('source', 'ì•Œ ìˆ˜ ì—†ëŠ” ì¶œì²˜')}")
                                else:
                                    st.write("- ì•Œ ìˆ˜ ì—†ëŠ” ì¶œì²˜")
                        else:
                            st.write("ì†ŒìŠ¤ ì •ë³´ë¥¼ í‘œì‹œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                # ëŒ€í™” ê¸°ë¡ì— ë„ìš°ë¯¸ ì‘ë‹µ ì¶”ê°€
                st.session_state.messages.append({"role": "assistant", "content": answer})
    # ëŒ€í™” ì´ˆê¸°í™” ë²„íŠ¼ ì¶”ê°€
    if st.button("ëŒ€í™” ì´ˆê¸°í™”"):
        reset_conversation()
if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        logging.error(f"ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹¤í–‰ ì¤‘ ì˜ˆê¸°ì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
        st.error("ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”.")
